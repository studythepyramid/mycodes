{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIqXtrNNWkisIsXXqQd08K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/studythepyramid/mycodes/blob/main/again_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyZNfb2LZgb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# %cd /content/gdrive/My Drive/Colab Notebooks/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import decimal\n",
        "\n",
        "big256bits = decimal.Decimal(2**256)\n",
        "print(f\"{big256bits:.4E}\")\n",
        "\n",
        "# rnn = nn.RNN(10, 20, 2)\n",
        "# input = torch.randn(5, 3, 10)\n",
        "# h0 = torch.randn(2, 3, 20)\n",
        "# output, hn = rnn(input, h0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpgwmrGjkhEf",
        "outputId": "c78431db-f0ab-4a20-e243-f44a6397bf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1579E+77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 20 # small?\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "# rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "# nn.Module?\n",
        "class RnnModel(nn.RNN):\n",
        "  def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "    super(RnnModel, self).__init__()\n",
        "    # self.hidden_dim = hidden_dim\n",
        "    # self.num_layers = num_layers\n",
        "\n",
        "    self.NLH = None\n",
        "    self.hidden = None\n",
        "    self.rnn = nn.RNN(\n",
        "        input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim) #X\n",
        "    self.lrelu = nn.LeakyReLU(0.5)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x[:,None,None]\n",
        "    if self.NLH == None: #?\n",
        "      self.NLH = x.shape\n",
        "\n",
        "    if self.hidden == None:\n",
        "      self.hidden = torch.zeros(num_layers, x.size(0), hidden_dim\n",
        "                     ).requires_grad_()\n",
        "\n",
        "    out, self.hidden = self.rnn(x, self.hidden.detach())\n",
        "    out = self.lrelu(out)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    out = self.lrelu\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self):\n",
        "    self.hidden = None;\n",
        "\n"
      ],
      "metadata": {
        "id": "z0BmZYD0L_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "oneRnn = RnnModel(input_dim, hidden_dim, num_layers, output_dim)\n",
        "xi = torch.randn(5, 1, 1)\n",
        "\n",
        "y = oneRnn(xi)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlBQDQeakpUg",
        "outputId": "5df58cd7-f44e-4ce0-e09f-2b8677542d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3770]],\n",
            "\n",
            "        [[0.4349]],\n",
            "\n",
            "        [[0.4427]],\n",
            "\n",
            "        [[0.4164]],\n",
            "\n",
            "        [[0.4649]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "# model = oneRnn\n",
        "# print(loss_func(model(xb), yb))\n",
        "\n",
        "def get_model(lr):\n",
        "    model = RnnModel(input_dim, hidden_dim, num_layers, output_dim)\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "model, opt = get_model(learning_rate)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSNlLgHf93ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loader and training?"
      ],
      "metadata": {
        "id": "SeF7n697HLMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "x = np.linspace(-20.0, 20.0, 500, dtype=np.float32)\n",
        "y = np.sin(x/3.14)\n",
        "# print(x.shape)\n",
        "# print(y.shape)\n",
        "\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "# train_ds = TensorDataset(x, y)\n",
        "# train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "\n",
        "x5 = x[:5]"
      ],
      "metadata": {
        "id": "KfS2w74hBjhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x5.view(-1, 1, 1).size(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3t3NhvyXvgG",
        "outputId": "d2455a24-5b5e-40f3-b800-39d865e1ca0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "    return x.view(-1, 1, 1) #?\n",
        "\n",
        "def shape_data(x):\n",
        "    return x.view(-1, 1, 1) #?\n",
        "\n",
        "#?\n",
        "model = nn.Sequential(\n",
        "    Lambda(preprocess),\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(4),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bi3t_PZaSyty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yahoo Finance data"
      ],
      "metadata": {
        "id": "kF9WExPBznxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "# Define the ticker symbol\n",
        "#ticker = 'AAPL'\n",
        "ticker = 'BTC-USD'\n",
        "\n",
        "# Get historical market data\n",
        "data = yf.download(ticker, start='2017-06-01', end='2024-12-31')\n",
        "\n",
        "# Display the first few rows of the data\n",
        "# print(data.head())"
      ],
      "metadata": {
        "id": "7i-cQaVNEoVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dlit = iter(train_dl)\n",
        "xi, yi = next(dlit)\n",
        "print(xi.shape)\n",
        "print(yi.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rdW3WNv1jql",
        "outputId": "fa949092-e9a4-4636-d55c-a53fcad9b748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):\n",
        "    for xb, yb in train_dl:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "pLptNWKuGUBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.1, 2.2], requires_grad=True)\n",
        "b = torch.tensor([3.3, 2.2], requires_grad=True)\n",
        "# loss_func(a, b)\n",
        "\n",
        "x5 = xt[:5, None, None]\n",
        "print(x5)\n",
        "print(x5.shape)\n",
        "p5 = model(x5)\n",
        "# print(p10)\n",
        "loss_func(p5.squeeze(), yt[:5])\n",
        "# p2 = p10.squeeze()\n",
        "# print(p2)\n",
        "# loss_func(p2, y10)\n",
        "# p2 - y10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXo6PjG_oGDN",
        "outputId": "4ba8df50-c938-4fbd-a754-75e95354977b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-20.0000]],\n",
            "\n",
            "        [[-19.9198]],\n",
            "\n",
            "        [[-19.8397]],\n",
            "\n",
            "        [[-19.7595]],\n",
            "\n",
            "        [[-19.6794]]])\n",
            "torch.Size([5, 1, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.6061, grad_fn=<DivBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xin = xt[:,None,None]\n",
        "pred = model(xin)\n",
        "\n",
        "loss = loss_func(pred.squeeze(), yt)\n",
        "\n",
        "loss.backward()\n",
        "opt.step()\n",
        "opt.zero_grad()"
      ],
      "metadata": {
        "id": "_Dz1vR2ttHYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xwP9TYib7gq",
        "outputId": "c3e61732-8496-46e5-8ba2-1234ab491d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(20.0573, grad_fn=<DivBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "SgORmtb8suKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic sine wave data\n",
        "t = np.linspace(-20.0, 20.0, 500, dtype=np.float32)\n",
        "fsin = np.sin(t/3.0)\n",
        "# plt.scatter(t, fsin)\n",
        "\n",
        "xt, yt = map(torch.tensor, (t, fsin))\n",
        "# xt, yt = xt.unsqueeze(1), yt.unsqueeze(1)\n",
        "print(xt.shape)\n",
        "print(yt.shape)\n",
        "\n",
        "x10 = xt[:10]\n",
        "y10 = yt[:10]\n",
        "xx = x10[:, None, None]\n",
        "print(xx, xx.shape)"
      ],
      "metadata": {
        "id": "3Z3mLsn1aDeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot1lE2_EHU8o",
        "outputId": "d4630cc1-ebd6-4256-8115-a75cabbc1d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods for variated input length for RNN"
      ],
      "metadata": {
        "id": "jXIAQ_ye6DUO"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class VariableLengthRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(VariableLengthRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Pad the input sequences\n",
        "        x_padded = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "\n",
        "        # Pack the padded sequences\n",
        "        packed_input = pack_padded_sequence(x_padded, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the packed sequence through the RNN\n",
        "        packed_output, hn = self.rnn(packed_input)\n",
        "\n",
        "        # Unpack the output (if needed)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Apply the fully connected layer\n",
        "        out = self.fc(output[:, -1, :])  # Get the last hidden state\n",
        "        return out\n",
        "\n",
        "# Example Usage:\n",
        "# Assume 'sequences' is a list of variable-length tensors\n",
        "sequences = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6])]\n",
        "lengths = [len(seq) for seq in sequences]  # Get lengths of sequences\n",
        "\n",
        "# Create an instance of the model\n",
        "model = VariableLengthRNN(input_dim=1, hidden_dim=10, num_layers=1, output_dim=1)\n",
        "\n",
        "# Forward pass\n",
        "output = model(sequences, lengths)\n",
        "\n",
        "# ... (rest of your training loop)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hw5uWQip3uyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mis LSTM Model"
      ],
      "metadata": {
        "id": "HRQR4k8-eBNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## https://medium.com/swlh/stock-price-prediction-with-pytorch-37f52ae84632\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "zPYcTZffeGcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}